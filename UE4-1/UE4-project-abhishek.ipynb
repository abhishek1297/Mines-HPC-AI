{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452857dc",
   "metadata": {},
   "source": [
    "# Probability and Statistics Project\n",
    "\n",
    "## Abhishek PURANDARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c668e91",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "## 1.1-1.5 Parameter estimation of the gamma distribution\n",
    "\n",
    "1. A gamma distribution with parameters a, and b is given as\n",
    "\n",
    "$$P(X|a,b) = \\dfrac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx}$$\n",
    "\n",
    "2. Since $x_i \\forall i$  are i.i.d random variables.\n",
    "They share the same parameters a, b.\n",
    "Therefore, their joint probability distribution is given as, $$P(x_1,...,x_n|a,b) = \\prod_{i=1}^{n} \\dfrac{b^a}{\\Gamma(a)}x_i^{a-1}e^{-bx_i}$$\n",
    "3. This can be a likelihood function $L(a,b|X) = P(X|a,b)$\n",
    "4. The log-likelihood function is $$\\ell(a,b|X) = \\log L(a,b|X)$$\n",
    "\n",
    "$$\n",
    "= \\log {\\left(\\prod_{i=1}^{n} \\dfrac{b^a}{\\Gamma(a)}x_i^{a-1}e^{-bx_i}\\right)} \\\\\n",
    "= \\sum_{i=1}^{n} \\log{\\left( \\dfrac{b^a}{\\Gamma(a)}x_i^{a-1}e^{-bx_i}\\right)} \\\\\n",
    "= \\sum_{i=1}^{n} \\left(a\\log b -\\log \\Gamma(a) + (a-1) \\log x_i - b x_i\\right) \\\\\n",
    "= n\\left(a \\log b - \\log \\Gamma(a) \\right) + (a-1) \\sum_{i=1}^{n} \\log x_i - b\\sum_{i=1}^{n} x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a47383",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "\n",
    "plt.rc('font', size=10)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=30)     # fontsize of the axes title\n",
    "\n",
    "# gamma distribution log-likelihood\n",
    "def log_likelihood(a, b, X):\n",
    "    return len(X) * (a * math.log(b) - math.log(math.gamma(a))) + \\\n",
    "    (a-1) * np.sum(np.log(X)) - b * np.sum(X)\n",
    "\n",
    "def visualize_log_likelihood(ax, a_true, b_true, data):\n",
    "    \n",
    "    X = np.arange(0.2, 5.0, 0.2)\n",
    "    Y = np.arange(0.2, 5.0, 0.2)\n",
    "    \n",
    "    X_, Y_ = np.meshgrid(X, Y)\n",
    "    Z_ = np.zeros(X_.shape)\n",
    "    tmp = float(\"-inf\")\n",
    "    i_m, j_m = 0, 0\n",
    "    for i in range(Z_.shape[0]):\n",
    "        for j in range(Z_.shape[1]):\n",
    "            Z_[i, j] = log_likelihood(X_[i, j], Y_[i, j], data)\n",
    "            if tmp <= Z_[i, j]:\n",
    "                i_m, j_m = i, j\n",
    "                tmp = Z_[i, j]\n",
    "    \n",
    "    ax.plot_surface(X_, Y_, Z_, cmap='viridis', edgecolor='black', alpha=0.4)\n",
    "    ax.scatter(a_true, b_true, log_likelihood(a_true, b_true, data), marker=\"o\", color=\"red\", s=500)\n",
    "    ax.scatter(X_[i_m, j_m], Y_[i_m, j_m], log_likelihood(X_[i_m, j_m], Y_[i_m, j_m], data), marker=\"^\", color=\"blue\", s=500)\n",
    "    ax.set_title(f\"a={a_true:.2f}, b={b_true:.2f}, N={len(data):,}\");\n",
    "    ax.set_xlabel(\"a\")\n",
    "    ax.set_ylabel(\"b\")\n",
    "    ax.set_zlabel(\"log-l\")\n",
    "    ax.xaxis.label.set_fontsize(35)\n",
    "    ax.yaxis.label.set_fontsize(35)\n",
    "    ax.zaxis.label.set_fontsize(35)\n",
    "    ax.set_zticks([])\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87adf156",
   "metadata": {},
   "source": [
    "### Log-likelihood for a = 2 and b = 2 with increasing N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87da205",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true = 2\n",
    "b_true = 2\n",
    "scale = 1 / b_true\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.gamma(shape=a_true, scale=scale, size=100)\n",
    "log_likelihood(a_true, b_true, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122381a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 4\n",
    "\n",
    "a_true = 2\n",
    "b_true = 2\n",
    "scale = 1 / b_true\n",
    "\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(30,25), subplot_kw=dict(projection='3d'), squeeze=False)\n",
    "fig.suptitle(\"4 plots for different N with 3 views. Red spot is the true likelihood and Blue triangle is the Maxima\", fontsize=35, y=0.01)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "for i in range(ncols):\n",
    "    \n",
    "    N = 10 ** (i+2)\n",
    "    X = np.random.gamma(shape=a_true, scale=scale, size=N)\n",
    "    true_l = log_likelihood(a_true, b_true, X)\n",
    "    ax[0,i] = visualize_log_likelihood(ax[0,i], a_true, b_true, X)\n",
    "    ax[0,i].view_init(20, 100)\n",
    "    ax[1,i] = visualize_log_likelihood(ax[1,i], a_true, b_true, X)\n",
    "    ax[1,i].view_init(100, 270)\n",
    "    ax[2,i] = visualize_log_likelihood(ax[2,i], a_true, b_true, X)\n",
    "    ax[2,i].set_xlim(a_true - 2, a_true + 2)\n",
    "    ax[2,i].set_ylim(b_true - 2, b_true + 2)\n",
    "    ax[2,i].set_zlim(true_l - int(0.3 * N), true_l + int(0.3 * N))\n",
    "    ax[2,i].view_init(0, 100)\n",
    "\n",
    "fig.tight_layout(pad=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae2de6",
   "metadata": {},
   "source": [
    "### Log-likelihood for random a and b with increasing N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 4\n",
    "\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(30,25), subplot_kw=dict(projection='3d'), squeeze=False)\n",
    "fig.suptitle(\"4 plots for different N with 3 views. Red spot is the true likelihood and Blue triangle is the Maxima\", fontsize=35, y=0.01)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "for i in range(ncols):\n",
    "    \n",
    "    N = 10 ** (i + 2)\n",
    "    \n",
    "    a_true = np.random.uniform(0.5, 3)\n",
    "    b_true = np.random.uniform(0.5, 3)\n",
    "    scale = 1 / b_true\n",
    "    \n",
    "    X = np.random.gamma(shape=a_true, scale=scale, size=N)\n",
    "    true_l = log_likelihood(a_true, b_true, X)\n",
    "    ax[0,i] = visualize_log_likelihood(ax[0,i], a_true, b_true, X)\n",
    "    ax[0,i].view_init(20, 270)\n",
    "    ax[1,i] = visualize_log_likelihood(ax[1,i], a_true, b_true, X)\n",
    "    ax[1,i].view_init(100, 270)\n",
    "    ax[2,i] = visualize_log_likelihood(ax[2,i], a_true, b_true, X)\n",
    "    ax[2,i].set_xlim(a_true - 2, a_true + 2)\n",
    "    ax[2,i].set_ylim(b_true - 2, b_true + 2)\n",
    "    ax[2,i].set_zlim(true_l - int(0.3 * N), true_l + int(0.3 * N))\n",
    "    ax[2,i].view_init(0, 100)\n",
    "\n",
    "fig.tight_layout(pad=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a013f9",
   "metadata": {},
   "source": [
    "#### What is the typical shape of the log-likelihood function?\n",
    "It is a concave function that decreases as we go away from the true parameters. The yellow region represents the convergence near the maximum likelihood.\n",
    "\n",
    "#### How many critical points does it have?\n",
    "There is only one critical point represented by the blue triangle.\n",
    "\n",
    "#### Where is the maximum of the function apparently located ?\n",
    "The maximum is located at $(\\hat{a}, \\hat{b})$ where $\\hat{b} \\approx b, \\hat{a} \\approx a$. The log-likelihood maxima is very close to the true parameters when N gets larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7d11f",
   "metadata": {},
   "source": [
    "## 1.6-1.12 Newton-Rhapson Method for MLE\n",
    "\n",
    "#### 1. Partial derivative w.r.t $a$\n",
    "$$\\frac{\\partial \\ell(a,b)}{\\partial a} = \\frac{\\partial}{\\partial a} \\left(n\\left(a \\log b - \\log \\Gamma(a) \\right) + (a-1) \\sum_{i=1}^{n} \\log x_i - b\\sum_{i=1}^{n} x_i \\right)$$\n",
    "\n",
    "$$ = n\\left(\\log b\\frac{\\partial}{\\partial a}a  - \\frac{\\partial}{\\partial a} (\\log \\Gamma(a))\\right) + \\sum_{i=1}^{n} \\log x_i \\frac{\\partial}{\\partial a}(a - 1) - \\frac{\\partial}{\\partial a} b\\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "The first-order derivative of $\\log \\Gamma(a)$ is a digamma function. We can represent this as $(\\log \\Gamma(a))'$\n",
    "Therefore,\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\ell(a,b)}{\\partial a} = n\\left(\\log b  - (\\log \\Gamma(a))'\\right) + \\sum_{i=1}^{n} \\log x_i$$\n",
    "\n",
    "\n",
    "#### 2. Partial derivative w.r.t $b$\n",
    "$$\\frac{\\partial \\ell(a,b)}{\\partial b} = \\frac{\\partial}{\\partial b} \\left(n\\left(a \\log b - \\log \\Gamma(a) \\right) + (a-1) \\sum_{i=1}^{n} \\log x_i - b\\sum_{i=1}^{n} x_i \\right)$$\n",
    "\n",
    "$$= n\\left(a\\frac{\\partial}{\\partial b}\\log b  - \\frac{\\partial}{\\partial b} (\\log \\Gamma(a))\\right) + \\frac{\\partial}{\\partial b}(a - 1)\\sum_{i=1}^{n} \\log x_i  - \\frac{\\partial}{\\partial b} b\\sum_{i=1}^{n} x_i$$\n",
    "Therefore,\n",
    "$$\\frac{\\partial \\ell(a,b)}{\\partial b} = \\frac{na}{b} - \\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "#### 3. To apply Newton-Rhapson method on multiple parameters we use Hessian Matrix $\\mathbb{H}(a, b)$ to store the partial derivatives\n",
    "\n",
    "$$\\Theta_{t+1} = \\Theta_{t} - [\\mathbb{H}(a,b)]^{-1} \\Delta L(a,b)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Theta_{t} = \\begin{pmatrix}\n",
    "a_t \\\\\n",
    "b_t\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Delta L(a,b) = \\begin{pmatrix}\n",
    "\\frac{\\partial \\ell(a,b)}{\\partial a} \\\\\n",
    "\\frac{\\partial \\ell(a,b)}{\\partial b}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "$$\\mathbb{H}(a,b) = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial^2 \\ell(a,b)}{\\partial^2 a} & \\frac{\\partial^2 \\ell(a,b)}{\\partial a\\partial b} \\\\\n",
    "\\frac{\\partial^2 \\ell(a,b)}{\\partial a\\partial b} & \\frac{\\partial^2 \\ell(a,b)}{\\partial^2 b}\n",
    "\\end{pmatrix}$$\n",
    "<br>\n",
    "\n",
    "The second-order derivative of $\\log \\Gamma(a)$ is a trigamma function. We can represent this as $(\\log \\Gamma(a))''$\n",
    "<br><br>\n",
    "\n",
    "$$\\frac{\\partial^2 \\ell(a,b)}{\\partial^2 a} = n(\\log \\Gamma(a))'', \\frac{\\partial^2 \\ell(a,b)}{\\partial a\\partial b} = \\frac{n}{b}, \\frac{\\partial^2 \\ell(a,b)}{\\partial^2 b} = -\\frac{na}{b^2}$$\n",
    "\n",
    "Therefore hessian inverse is,\n",
    "\n",
    "$$\n",
    "[\\mathbb{H}(a,b)]^{-1} = \\frac{1}{n(1 - a(\\log \\Gamma(a))'')}\n",
    "\\begin{pmatrix}\n",
    "a & b \\\\\n",
    "b & b^2(\\log \\Gamma(a))''\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008064d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_newton(a, b, X):\n",
    "    \n",
    "    from scipy.special import polygamma\n",
    "    \n",
    "    def digamma(a):\n",
    "        return polygamma(0, a)\n",
    "    \n",
    "    def trigamma(a):\n",
    "        return polygamma(1, a)\n",
    "    \n",
    "    def H_inv(a, b, n):\n",
    "        return (1/(n * (1 - a * trigamma(a))) * np.array([\n",
    "                                                    [a, b],\n",
    "                                                    [b, (b**2) * trigamma(a)]\n",
    "                                            ])\n",
    "               )\n",
    "    def delta_l_T(a, b, X, n):\n",
    "        return np.array([\n",
    "                    [n * math.log(b) - n * digamma(a) + np.sum(np.log(X))],\n",
    "                    [n*a/b - np.sum(X)]\n",
    "                ])\n",
    "    \n",
    "    n = len(X)\n",
    "    at_bt_T = np.array([[a], [b]])\n",
    "    \n",
    "    res = np.subtract(at_bt_T, np.matmul(H_inv(a, b, n), delta_l_T(a, b, X, n)))\n",
    "    \n",
    "    return res[:,0]\n",
    "\n",
    "\n",
    "def MLE_gamma(a_init, b_init, X, tolerance=1e-3, max_iters=100):\n",
    "    \n",
    "    err = 1.0\n",
    "    a, b = a_init, b_init\n",
    "    lik = log_likelihood(a, b, X)\n",
    "    i = 0\n",
    "    \n",
    "    while (i < max_iters and err > tolerance):\n",
    "        \n",
    "        a, b = update_newton(a, b, X)\n",
    "        if a < 0 or b < 0:\n",
    "            i = -1\n",
    "            break\n",
    "        lik_new = log_likelihood(a, b, X)\n",
    "        err = abs((lik_new - lik) / lik)\n",
    "        lik = lik_new\n",
    "        i += 1\n",
    "\n",
    "    return a, b, i\n",
    "\n",
    "def get_str(run_id, a_init, b_init, a_new, b_new, iters):\n",
    "    return f\"\\n| {run_id} | {a_init:.2f} | {b_init:.2f} | {a_new:.2f} | {b_new:.2f} | {iters} |\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown\n",
    "\n",
    "n = 10\n",
    "a_true = 2\n",
    "b_true = 2\n",
    "X = np.random.gamma(shape=a_true, scale=1/b_true, size=int(1e5))\n",
    "delta = np.linspace(-1, 3, n)\n",
    "\n",
    "display_markdown(f\"### a_true = {a_true}, b_true = {b_true}\", raw=True)\n",
    "\n",
    "a_new, b_new, iters = MLE_gamma(a_true, b_true, X, tolerance=1e-3)\n",
    "\n",
    "table = \"| run | a_init | b_init | a_new | b_new | iterations |\\n|---|---|---|---|---|---|\"\n",
    "table += get_str(\"trial\", a_true, b_true, a_new, b_new, iters)\n",
    "\n",
    "for i in range(n):\n",
    "    a, b = a_true + delta[i], b_true + delta[i]\n",
    "    a_new, b_new, iters = MLE_gamma(a, b, X)\n",
    "    table += get_str(i+1, a, b, a_new, b_new, iters)\n",
    "\n",
    "display_markdown(table, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccae4e0",
   "metadata": {},
   "source": [
    "## 1.13-1.15 Method of Moments Estimator\n",
    "\n",
    "For the gamma distribution $X \\sim Gamma(a, b)$, the first moment is given by $\\mathbb{E}(X) = \\frac{a}{b}$ and the second moment is given by $\\mathbb{E}(X^2) = \\frac{a(a+1)}{b^2}$\n",
    "\n",
    "\n",
    "The first moment is $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$ and second moment can is $\\bar{X^2} = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2$\n",
    "\n",
    "$$\\frac{a}{b} =  \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n",
    "$$\\frac{a(a+1)}{b^2} = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2$$\n",
    "From equation 1 we can get\n",
    "$$a = b\\bar{X}$$\n",
    "Substituting this value in the second moment's equation implies that,\n",
    "$$\\frac{b\\bar{X}(b\\bar{X}+1)}{b^2} = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2$$\n",
    "\n",
    "By solving this we get,\n",
    "$$\\hat{b} = \\frac{\\bar{X}}{\\frac{1}{n} \\sum_{i=1}^{n} \\left(x_i^2 - \\bar{X}^2\\right)}$$\n",
    "Therefore for $\\hat{a}$,\n",
    "$$\\hat{a} = \\hat{b}\\bar{X}$$\n",
    "\n",
    "$$\\hat{a} = \\frac{\\bar{X}^2}{\\frac{1}{n} \\sum_{i=1}^{n} \\left(x_i^2 - \\bar{X}^2\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MME_gamma(X):\n",
    "\n",
    "    def get_moment(X, n):\n",
    "        return np.array([x**n for x in X]).mean()\n",
    "    \n",
    "    m1 = get_moment(X, 1)       \n",
    "    m2 = get_moment(X, 2)       \n",
    "    inv = 1 / (m2 - m1**2)\n",
    "    a = (m1**2) * inv\n",
    "    b = m1 * inv\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d62553",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true = 2\n",
    "b_true = 2\n",
    "scale = 1 / b_true\n",
    "X = np.random.gamma(shape=a_true, scale=scale, size=int(1e3))\n",
    "\n",
    "a_hat, b_hat = MME_gamma(X)\n",
    "a_hat2, b_hat2, iters = MLE_gamma(a_hat, b_hat, X, tolerance=1e-6)\n",
    "\n",
    "display_markdown(f\"#### a_true = {a_true}, b_true = {b_true}\", raw=True)\n",
    "display_markdown(f\"#### MME Gamma: a = {a_hat:.2f}, b = {b_hat:.2f}\", raw=True)\n",
    "display_markdown(f\"#### MLE Gamma: a = {a_hat2:.2f}, b = {b_hat2:.2f}, iterations = {iters}\", raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c9e87",
   "metadata": {},
   "source": [
    "## 1.16-1.17 MLE vs MME\n",
    "\n",
    "### Evaluation of the estimators\n",
    "Let $\\hat\\theta$ be an estimator of the random variables, given that we have observed the random variable $X$. The mean squared error (MSE) of this estimator against the true parameter $\\theta$ is defined as\n",
    "$$MSE(\\hat\\theta, \\theta) = \\mathbb{E}[||\\hat\\theta - \\theta||^2] \\\\\n",
    " = Var(\\hat\\theta) + Bias^2(\\hat\\theta, \\theta)$$\n",
    "<br>\n",
    "\n",
    "The following code computes the MSE over large number of data sets $T = 1000$ of fixed sized $N [10^2, 10^5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "from tqdm import tqdm\n",
    "\n",
    "iters = 0\n",
    "T = 1000\n",
    "support = []\n",
    "mse = {\n",
    "    \"mme\": {\"a\": [], \"b\": []},\n",
    "    \"mle\": {\"a\": [], \"b\": []},\n",
    "}\n",
    "\n",
    "print(f\"Running for {T} iterations per N samples\")\n",
    "\n",
    "for i in tqdm(range(2, 6)):\n",
    "\n",
    "    mme_stats =  {\"a\": 0, \"b\": 0, \"mse_a\": [], \"mse_b\": []}\n",
    "    mle_stats =  {\"a\": 0, \"b\": 0, \"mse_a\": [], \"mse_b\": []}\n",
    "    \n",
    "    N = 10 ** i\n",
    "    support.append(N)\n",
    "    for _ in range(T):\n",
    "    \n",
    "        a_true = np.random.uniform(2, 2.5)\n",
    "        b_true = np.random.uniform(2, 2.5)\n",
    "        X = np.random.gamma(shape=a_true, scale=1/b_true, size=N)\n",
    "        a_hat_mme, b_hat_mme = MME_gamma(X)\n",
    "        a_hat_mle, b_hat_mle, iters = MLE_gamma(1.5, 1.5, X)\n",
    "        if iters < 0:\n",
    "            print(f\"negatives found stopping at {T}\", a_true, b_true)\n",
    "            break\n",
    "        mme_stats[\"a\"] += (a_true-a_hat_mme)**2\n",
    "        mme_stats[\"b\"] += (b_true-b_hat_mme)**2\n",
    "        mle_stats[\"a\"] += (a_true-a_hat_mle)**2\n",
    "        mle_stats[\"b\"] += (b_true-b_hat_mle)**2\n",
    "    if iters < 0:\n",
    "        break\n",
    "    mse[\"mme\"][\"a\"].append(mme_stats[\"a\"]/T)\n",
    "    mse[\"mme\"][\"b\"].append(mme_stats[\"b\"]/T)\n",
    "    mse[\"mle\"][\"a\"].append(mle_stats[\"a\"]/T)\n",
    "    mse[\"mle\"][\"b\"].append(mle_stats[\"b\"]/T)\n",
    "\n",
    "if iters >= 0: #negative not found\n",
    "    \n",
    "    plt.rc('font', size=10)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=10)     # fontsize of the axes title\n",
    "\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(16,6))\n",
    "\n",
    "    for i, p in enumerate([\"a\", \"b\"]):\n",
    "        ax[i].set_title(f\"Parameter {p}\", fontsize=15)\n",
    "        ax[i].plot(support, mse[\"mme\"][p])\n",
    "        ax[i].plot(support, mse[\"mle\"][p])\n",
    "        ax[i].set_xscale(\"log\")\n",
    "        ax[i].set_xlabel(\"N samples\")\n",
    "        ax[i].set_ylabel(\"MSE\")\n",
    "        ax[i].legend([\"Moments\", \"Log-likelihood\"], fontsize=12)\n",
    "        ax[i].xaxis.label.set_fontsize(15)\n",
    "        ax[i].yaxis.label.set_fontsize(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff6ecd",
   "metadata": {},
   "source": [
    "We can see from the graphs that $ MSE(\\hat\\theta, \\theta) \\to 0 $ as $ N \\to \\infty $\n",
    "\n",
    "**MLE_gamma** is more accurate relative to  **MME_gamma**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922804c",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "## 2.1 Simulation using Metropolis-Hastings algorithm\n",
    "\n",
    "1. We have random variables $$X \\sim Poisson(\\theta)$$\n",
    "\n",
    "2. Therefore, Likelihood for $\\theta$ will be\n",
    "\n",
    "$$L(\\theta|X) = \\prod_{i=1}^n \\dfrac{e^{-\\theta}\\theta^{x_i}}{x_i!}$$\n",
    "\n",
    "3. We assume a prior distribution for $\\theta$ is $\\pi(\\theta) \\propto G(k, \\lambda)$ where $k$ and $\\lambda$ are known parameters. Therefore,\n",
    "\n",
    "$$\\pi(\\theta) = \\dfrac{\\lambda^{k}}{\\Gamma(k)}\\theta^{k-1}e^{-\\lambda\\theta}, \\theta > 0$$\n",
    "\n",
    "4. Posterior of $\\theta$ will be $\\pi(\\theta|X) \\propto L(\\theta)\\pi(\\theta)$\n",
    "\n",
    "$$\\pi(\\theta|X) \\propto \\theta^{\\sum x_i + k - 1} e^{-(n+\\lambda)\\theta}, \\theta > 0$$\n",
    "<br>\n",
    "\n",
    "$$\\pi(\\theta|X) \\propto Gamma(k + \\sum x_i, \\lambda + n),  n = |x|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899ad62",
   "metadata": {},
   "source": [
    "\n",
    "1. Consider known parameters $k = 1$, $\\lambda=2.95$, $x = 10$\n",
    "\n",
    "2. The posterior probability is $$\\pi(\\theta|X=x) \\approx Gamma(a, b), a=k + x, b = \\lambda + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d858898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import math\n",
    "np.seterr(all='raise')\n",
    "\n",
    "    \n",
    "# Given parameters\n",
    "X = [10]\n",
    "a = (1 + sum(X))\n",
    "b = (2.95 + len(X))\n",
    "\n",
    "# Posterior Gamma Likelihood * Prior\n",
    "def posterior(theta):\n",
    "    return (theta ** (a - 1)) * math.exp(-b*theta)\n",
    "\n",
    "# general metropolis-hastings algorithm\n",
    "# the function returns the second half of the samples\n",
    "def metropolis_hastings(x_init, posterior, N, positive_only=False):\n",
    "    \n",
    "    # init support and its prob\n",
    "    x = x_init\n",
    "    p = posterior(x)\n",
    "    accepted = 0\n",
    "    s = 10\n",
    "    samples = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        xn = x + np.random.normal()\n",
    "#         xn = x * np.random.lognormal()\n",
    "        if positive_only:\n",
    "            xn = abs(xn)\n",
    "        pn = posterior(xn)\n",
    "        u = np.random.uniform(0, 1, 1)\n",
    "        alpha = min(pn/p, 1)\n",
    "        if alpha > u:\n",
    "            if x != xn:\n",
    "                accepted += 1\n",
    "            p = pn\n",
    "            x = xn\n",
    "        if i % s == 0:\n",
    "            samples.append(x)\n",
    "    \n",
    "    burn_in = int(len(samples) * 0.5)\n",
    "    return samples[burn_in:], (accepted/N)\n",
    "\n",
    "\n",
    "support = np.linspace(0.0, 10.0, 1000)\n",
    "pdf = np.asarray([posterior(x) for x in support])\n",
    "\n",
    "N = 10_00_000\n",
    "lmbda_init = 1.5 #starting point\n",
    "positives = True\n",
    "samples, acceptance_rate = metropolis_hastings(lmbda_init, posterior, N, positives)\n",
    "samples = np.array(samples)\n",
    "\n",
    "    \n",
    "plt.rc('font', size=10)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=15)     # fontsize of the axes title\n",
    "\n",
    "nrows = 1\n",
    "ncols = 2\n",
    "\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(16,6))\n",
    "\n",
    "ax[0].plot(support, pdf)\n",
    "ax[0].set_xlabel(\"X\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].set_title('Metropolis Hastings Posterior for Gamma')\n",
    "\n",
    "ax[1].plot(support, pdf)\n",
    "ax[1].hist(samples, bins=50, density=True, edgecolor=\"black\", linewidth=0.1)\n",
    "ax[1].legend([\"Posterior\", \"Sampled posterior\"])\n",
    "ax[1].set_xlabel(\"X\")\n",
    "ax[1].set_ylabel(\"Density\")\n",
    "ax[1].set_title(f'MCMC simulation N={N:,} acceptance={acceptance_rate:.2f}')\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].xaxis.label.set_fontsize(15)\n",
    "    ax[i].yaxis.label.set_fontsize(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b416cff",
   "metadata": {},
   "source": [
    "## 2.2 Kernel Density Estimation\n",
    "\n",
    "1. Using Gaussian distribution for kernel density estimation for the above histogram and then calculating the Mean squared error of true density against the evaluated density.\n",
    "\n",
    "2. Not applying any bandwidth as the fit is nearly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "kde = gaussian_kde(samples)\n",
    "estimated = kde.evaluate(support)\n",
    "mse = mean_squared_error(pdf, estimated)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(support, pdf)\n",
    "plt.plot(support, estimated, alpha=0.8)\n",
    "plt.title(f\"Mean Squared Error={mse:.3e}\")\n",
    "plt.legend([\"truth\", \"estimated\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
